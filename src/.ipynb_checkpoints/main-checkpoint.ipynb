{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b16d4aa-f00c-4fec-916c-21a52d550103",
   "metadata": {},
   "source": [
    "<h2><center>Graduate Project - CSCI 6350</center></h2>\n",
    "<h1><center>Comparing Deep Learning Transformers To Naive Bayes and Random Forest Models in the Context of Emotion Detection</center></h1>\n",
    "<h3><center>Richard Hoehn</center></h3>\n",
    "<h3><center>Middle Tennessee State University</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf19491-dff7-4fb4-b617-f580f62e38e3",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "- Getting NumPy, Pandas, MatPlotLib, StatsModels and Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0948e6e-c93e-47cb-bfd0-65566b0b0a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports of main Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Used for Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "\n",
    "# BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# Create PyTorch dataset and dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "#style options\n",
    "%matplotlib inline\n",
    "\n",
    "# If you want graphs to automatically display without plt.show\n",
    "pd.set_option('display.max_columns', 500) # Allows for up to 500 columns to be displayed when viewing a dataframe\n",
    "pd.set_option('display.max_info_columns', 250) # Allows for up to 250 columns to be displayed when viewing a dataframe as \"info()\"\n",
    "\n",
    "# Note: I got an error following the homework details, since the general seaborn style lib has been discontinued. I used the \"-v0_8\" instead\n",
    "# as suggested in stackoverflow: https://stackoverflow.com/questions/74716259/the-seaborn-styles-shipped-by-matplotlib-are-deprecated-since-3-6\n",
    "plt.style.use('seaborn-v0_8') # A style that can be used for plots - see style refere nce above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ae1d8-2ae4-439c-b45d-e795e9333b95",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ac22b9e-df89-4438-9494-fbfac805f288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Emotion Tagged Sentences ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion_en\n",
       "neutral       360\n",
       "worry         334\n",
       "sadness       235\n",
       "happiness     225\n",
       "love          168\n",
       "surprise       91\n",
       "hate           51\n",
       "enthusiasm     28\n",
       "anger           8\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_en</th>\n",
       "      <th>sentence_en</th>\n",
       "      <th>emotion_de</th>\n",
       "      <th>sentence_de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>sorry mum, will treat you well from now on</td>\n",
       "      <td>Traurigkeit</td>\n",
       "      <td>Entschuldigung, Mama, werde Sie von nun an gut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate snoring. remind me if my future husband...</td>\n",
       "      <td>Ekel</td>\n",
       "      <td>Ich hasse es zu schnarchen.Erinnern Sie mich d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surprise</td>\n",
       "      <td>And I end up in privilege... Oh well, at least...</td>\n",
       "      <td>Überraschung</td>\n",
       "      <td>Und ich lande im Privileg ... na ja, zumindest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worry</td>\n",
       "      <td>Tasha's really bad haircut. She's being treate...</td>\n",
       "      <td>Angst</td>\n",
       "      <td>Tasha ist wirklich schlechter Haarschnitt.Sie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I miss my mom..  &amp;quot;May angels lead you in&amp;...</td>\n",
       "      <td>Traurigkeit</td>\n",
       "      <td>Ich vermisse meine Mutter.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_en                                        sentence_en    emotion_de  \\\n",
       "0    sadness         sorry mum, will treat you well from now on   Traurigkeit   \n",
       "1       hate  i hate snoring. remind me if my future husband...          Ekel   \n",
       "2   surprise  And I end up in privilege... Oh well, at least...  Überraschung   \n",
       "3      worry  Tasha's really bad haircut. She's being treate...         Angst   \n",
       "4    sadness  I miss my mom..  &quot;May angels lead you in&...   Traurigkeit   \n",
       "\n",
       "                                         sentence_de  \n",
       "0  Entschuldigung, Mama, werde Sie von nun an gut...  \n",
       "1  Ich hasse es zu schnarchen.Erinnern Sie mich d...  \n",
       "2  Und ich lande im Privileg ... na ja, zumindest...  \n",
       "3  Tasha ist wirklich schlechter Haarschnitt.Sie ...  \n",
       "4                         Ich vermisse meine Mutter.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load csv into dataframe\n",
    "df =  pd.read_csv('pd_en_translated.csv', index_col = None, header=0)\n",
    "\n",
    "# Display Some details of the Dataset\n",
    "print(\"\\n*** Emotion Tagged Sentences ***\")\n",
    "display(df[\"emotion_en\"].value_counts())\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33731d7d-0285-4e53-a8ff-4ae6784bb689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop Columns No Needed\n",
    "df.drop([\"emotion_de\", \"sentence_de\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15e507fc-f531-43b7-aa34-ae3eab306c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label Encoding the Emotions\n",
    "labelencoder = LabelEncoder()\n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "df['label'] = label_encoder.fit_transform(df['emotion_en']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a08cfe8-a514-4b30-9fe5-56e2e43f3f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_en</th>\n",
       "      <th>sentence_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>sorry mum, will treat you well from now on</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate snoring. remind me if my future husband...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surprise</td>\n",
       "      <td>And I end up in privilege... Oh well, at least...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worry</td>\n",
       "      <td>Tasha's really bad haircut. She's being treate...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I miss my mom..  &amp;quot;May angels lead you in&amp;...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>worry</td>\n",
       "      <td>@OfficialBabyV I wish you where going to be in...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I wouldn't mind some sunshine and a walk. Wher...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>love</td>\n",
       "      <td>@ddlovato I'm sure it was amazing  Wish I coul...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>love</td>\n",
       "      <td>Hanging with Megan Brooks. Best girl in the world</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>love</td>\n",
       "      <td>@MetroplexBaby: thx for the follow!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion_en                                        sentence_en  label\n",
       "0       sadness         sorry mum, will treat you well from now on      6\n",
       "1          hate  i hate snoring. remind me if my future husband...      3\n",
       "2      surprise  And I end up in privilege... Oh well, at least...      7\n",
       "3         worry  Tasha's really bad haircut. She's being treate...      8\n",
       "4       sadness  I miss my mom..  &quot;May angels lead you in&...      6\n",
       "...         ...                                                ...    ...\n",
       "1495      worry  @OfficialBabyV I wish you where going to be in...      8\n",
       "1496    sadness  I wouldn't mind some sunshine and a walk. Wher...      6\n",
       "1497       love  @ddlovato I'm sure it was amazing  Wish I coul...      4\n",
       "1498       love  Hanging with Megan Brooks. Best girl in the world      4\n",
       "1499       love                @MetroplexBaby: thx for the follow!      4\n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "351c4dd0-5b9e-4343-b717-0cfd76c4442b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/richardhoehn/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sentence before tokenization:  i hate snoring. remind me if my future husband ever snores, we're getting a divorce.\n",
      "Encoded Input from dataset:  [101, 1045, 5223, 1055, 12131, 2075, 1012, 10825, 2033, 2065, 2026, 2925, 3129, 2412, 1055, 12131, 2229, 1010, 2057, 1005, 2128, 2893, 1037, 8179, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Setup / Instantiate the Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "MAX_LEN = 256\n",
    "SEED = 42\n",
    "\n",
    "# Create the Level and Sentence List (Arrays)\n",
    "sentences = df.sentence_en.values\n",
    "labels    = df.label.values\n",
    "\n",
    "input_ids = [ tokenizer.encode(sent, add_special_tokens = True, max_length = MAX_LEN, pad_to_max_length = True) for sent in sentences ]\n",
    "\n",
    "\n",
    "print(\"Actual sentence before tokenization: \",sentences[1])\n",
    "print(\"Encoded Input from dataset: \",input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04d12851-4c86-4abb-b308-ffdb88ba0f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "## Create attention mask\n",
    "attention_masks = []\n",
    "\n",
    "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "\n",
    "print(attention_masks[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de4839-dd89-4ac2-939d-4f854fa111c6",
   "metadata": {},
   "source": [
    "# Split into a training set and a test set using a stratified k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48839765-4bef-44b3-9e9e-e2e650a1693c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs ,train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels,\n",
    "                                                                                    random_state = SEED,\n",
    "                                                                                    test_size = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                       input_ids,\n",
    "                                                       random_state = SEED,\n",
    "                                                       test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6fd4b76-b14d-4a95-ac5a-69a42a672e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/1546964466.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs = torch.tensor(train_inputs)\n",
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/1546964466.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_inputs = torch.tensor(validation_inputs)\n",
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/1546964466.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/1546964466.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_labels = torch.tensor(validation_labels)\n",
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/1546964466.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/1546964466.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_masks = torch.tensor(validation_masks)\n"
     ]
    }
   ],
   "source": [
    "# convert all our data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27efb5b6-399c-4102-8196-ea84c316e3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1051, 11923,  2008,  2052,  2022,  1001,  3968, 20559,  2102,\n",
       "          2692,  2683,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " tensor(7))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d05fb226-9902-41ba-a7b8-73ae05943ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "139b7463-e382-4e4d-8f6e-ea0521749d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data based on labels:  label\n",
      "5    360\n",
      "8    334\n",
      "6    235\n",
      "2    225\n",
      "4    168\n",
      "7     91\n",
      "3     51\n",
      "1     28\n",
      "0      8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of data based on labels: \",df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "150e7ce1-04fb-49a3-ad43-0576e6286aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9).to(device)\n",
    "\n",
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5cd3df15-3f76-4e97-b2fa-40671a6903a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_16401/3028283342.py:9: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1,epochs+1,desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c39929ed00401eb4216a594178bf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  1.3333333333333333e-05\n",
      "\n",
      "\tAverage Training loss: 1.9252452850341797\n",
      "\n",
      "\tValidation Accuracy: 0.2676136363636364\n",
      "\n",
      "\tValidation MCC Accuracy: 0.08826268308218893\n",
      "<====================== Epoch 2 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  6.666666666666667e-06\n",
      "\n",
      "\tAverage Training loss: 1.6921545710674553\n",
      "\n",
      "\tValidation Accuracy: 0.3693181818181818\n",
      "\n",
      "\tValidation MCC Accuracy: 0.22527461377959007\n",
      "<====================== Epoch 3 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n",
      "\n",
      "\tAverage Training loss: 1.4368068118428075\n",
      "\n",
      "\tValidation Accuracy: 0.4005681818181818\n",
      "\n",
      "\tValidation MCC Accuracy: 0.22913047379494622\n"
     ]
    }
   ],
   "source": [
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "  # Calculate total loss for this epoch\n",
    "  batch_loss = 0\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the norm of the gradients to 1.0\n",
    "    # Gradient clipping is not in AdamW anymore\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate schedule\n",
    "    scheduler.step()\n",
    "\n",
    "    # Clear the previous accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    batch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "  #store the current learning rate\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "    learning_rate.append(param_group['lr'])\n",
    "    \n",
    "  train_loss_set.append(avg_train_loss)\n",
    "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    \n",
    "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
    "    \n",
    "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
    "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d71921-ac2d-49d0-9bcc-d3ae886ef5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0eda25-102a-4f27-a2c2-5a36e39cd849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4991e5-b0b8-4c95-ba19-17851bda32c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988bfaf-8020-42d7-a193-8c6ab7d6b9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132dd911-1961-42a5-829d-aafdfb42d811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fc480-3715-4697-bb9c-7f6664f14f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ce59c-27eb-4b85-8af3-4df9d9a6dde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624c9d6-0f9a-4182-9293-8b5dbca7e200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e8890-c122-4116-bf17-791621dc2d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754b53e-6a63-4e6d-bcf1-10fdec2a1bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38f090-8496-4908-987d-fe92dc69f1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52910df7-b97a-4d26-9dbf-ade743430864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512f0db-89f1-4b51-9114-56eccc9bc8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc7d2-252f-4f3b-a984-bca8272dc4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff0e19-9b8c-4259-a727-7cf9cea6a279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df0417-51d5-46ce-a964-5eea3b9ebe1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15a56c-935b-4daf-a7a4-a278eb718125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c5925-702d-446e-98eb-96a1829a845c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc992374-ad1e-451d-b991-62a87e257e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "230ada5e-f07a-43ce-97b6-cc98f2afca25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert sentences to tensors\n",
    "tokenized_sentences = [torch.tensor(sentence) for sentence in encoded_sentences]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "padded_sequences = pad_sequence(tokenized_sentences, batch_first=True, padding_value=0)\n",
    "\n",
    "# Convert emotion_en_label to tensors\n",
    "labels = torch.tensor(df['emotion_en_label'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "# Create PyTorch dataset and dataloader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.sentences[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "dataset = MyDataset(padded_sequences, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1afc3b36-24e7-4089-8ac4-b8f9427d48fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6427e52-abf8-4797-b44d-b1fd1a870636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 0.00002)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b84f3980-bee0-415d-a88a-b577172626c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Training Loss: 1.9139\n",
      "Epoch 2/5, Average Training Loss: 1.9009\n",
      "Epoch 3/5, Average Training Loss: 1.8863\n",
      "Epoch 4/5, Average Training Loss: 1.9012\n",
      "Epoch 5/5, Average Training Loss: 1.8955\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdf9839b-50b7-432b-a33d-6efd4d08304f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m padded_sequences[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(padded_sequences[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "padded_sequences[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9c5dd93-c2ab-4c59-83ad-776143b48210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3b482b0-8b46-4e1d-a154-624ed4ac7dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5179404-ea5f-41ae-90e7-86ded9e34068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5dfb3ea3-b198-4184-a476-5c354e4f959b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:423\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_device_properties(device)\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:453\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8486fb2-4134-477c-8e37-30e3e5861ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
